{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5, Gruppe 8\n",
    "Fabian Meyer, Joel Salzmann, Cyrille Ulmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1\n",
    "\n",
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2dd44324060e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nltk"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk import tag\n",
    "from nltk import chunk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re, string, collections\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"macbeth.txt\", \"r\")\n",
    "text = file.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Bereinigung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first part of the text\n",
    "cleaned_text = text.split(\"David Reed\",1)[1] \n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satzzeichen entfernen\n",
    "text_without_puncuation = \"[\" + re.sub(\"\\.\",\"\", string.punctuation)+\"]\"\n",
    "cleaned_text = re.sub(text_without_puncuation,\"\", cleaned_text)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = cleaned_text.split()\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "final_tokenized_text = []\n",
    "\n",
    "for w in tokenized_text:\n",
    "    if not w in stop_words:\n",
    "        final_tokenized_text.append(w)\n",
    "        \n",
    "len(final_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "listBigrams = nltk.bigrams(final_tokenized_text)\n",
    "\n",
    "freq_bi = nltk.FreqDist(listBigrams)\n",
    "\n",
    "fdist_bigrams = nltk.FreqDist(freq_bi)\n",
    "\n",
    "sorted_bigrams = list(reversed(sorted(fdist_bigrams.items(), key=operator.itemgetter(1))))\n",
    "\n",
    "\n",
    "for k,v in fdist_bigrams:\n",
    "    print (k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "listTrigrams = nltk.trigrams(final_tokenized_text)\n",
    "freq_tri = nltk.FreqDist(listTrigrams)\n",
    "fdist_trigrams = nltk.FreqDist(freq_tri)\n",
    "\n",
    "sorted_trigrams = list(reversed(sorted(fdist_trigrams.items(), key=operator.itemgetter(1))))\n",
    "\n",
    "for k,v in sorted_trigrams:\n",
    "    print (k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Document 0  Document 1  Document 2\n",
      "alive            0.275692    0.000000    0.000000\n",
      "artifacts        0.000000    0.000000    0.286807\n",
      "bridge           0.000000    0.288675    0.000000\n",
      "build            0.000000    0.000000    0.286807\n",
      "carl             0.000000    0.000000    0.286807\n",
      "civilization     0.000000    0.000000    0.286807\n",
      "coma             0.275692    0.000000    0.000000\n",
      "communication    0.000000    0.288675    0.000000\n",
      "communities      0.000000    0.288675    0.000000\n",
      "construction     0.000000    0.288675    0.000000\n",
      "deputy           0.275692    0.000000    0.000000\n",
      "envisioned       0.000000    0.000000    0.286807\n",
      "facilitate       0.000000    0.288675    0.000000\n",
      "forces           0.000000    0.288675    0.000000\n",
      "gravely          0.000000    0.288675    0.000000\n",
      "grimes           0.275692    0.000000    0.000000\n",
      "group            0.209671    0.000000    0.218124\n",
      "injured          0.000000    0.288675    0.000000\n",
      "join             0.000000    0.288675    0.000000\n",
      "lead             0.275692    0.000000    0.000000\n",
      "learn            0.275692    0.000000    0.000000\n",
      "make             0.000000    0.000000    0.286807\n",
      "need             0.000000    0.000000    0.286807\n",
      "restore          0.000000    0.288675    0.000000\n",
      "rick             0.209671    0.000000    0.218124\n",
      "risky            0.000000    0.000000    0.286807\n",
      "ruins            0.275692    0.000000    0.000000\n",
      "run              0.000000    0.000000    0.286807\n",
      "search           0.000000    0.000000    0.286807\n",
      "sheriff          0.275692    0.000000    0.000000\n",
      "site             0.000000    0.288675    0.000000\n",
      "stay             0.275692    0.000000    0.000000\n",
      "survivors        0.275692    0.000000    0.000000\n",
      "trade            0.000000    0.288675    0.000000\n",
      "wakes            0.275692    0.000000    0.000000\n",
      "washington       0.000000    0.000000    0.286807\n",
      "world            0.275692    0.000000    0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open('IMDB_1.txt', 'r') as myfile:\n",
    "    imdb1=myfile.read().replace('\\n', '')\n",
    "with open('IMDB_2.txt', 'r') as myfile:\n",
    "    imdb2=myfile.read().replace('\\n', '')\n",
    "with open('IMDB_3.txt', 'r') as myfile:\n",
    "    imdb3=myfile.read().replace('\\n', '')    \n",
    "\n",
    "documents = [\n",
    "    imdb1,\n",
    "    imdb2,\n",
    "    imdb3\n",
    "]\n",
    "\n",
    "document_names = ['Document {:d}'.format(i) for i in range(len(documents))]\n",
    "\n",
    "def get_tfidf(docs, ngram_range=(1,1), index=None):\n",
    "    vect = TfidfVectorizer(stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf = vect.fit_transform(docs).todense()\n",
    "    return pd.DataFrame(tfidf, columns=vect.get_feature_names(), index=index).T\n",
    "\n",
    "# 1-gram\n",
    "print(get_tfidf(documents, ngram_range=(1,1), index=document_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Document 0  Document 1  Document 2\n",
      "artifacts need               0.00000    0.000000    0.288675\n",
      "bridge facilitate            0.00000    0.301511    0.000000\n",
      "build civilization           0.00000    0.000000    0.288675\n",
      "carl envisioned              0.00000    0.000000    0.288675\n",
      "civilization carl            0.00000    0.000000    0.288675\n",
      "coma learn                   0.27735    0.000000    0.000000\n",
      "communication trade          0.00000    0.301511    0.000000\n",
      "communities join             0.00000    0.301511    0.000000\n",
      "construction site            0.00000    0.301511    0.000000\n",
      "deputy rick                  0.27735    0.000000    0.000000\n",
      "facilitate communication     0.00000    0.301511    0.000000\n",
      "forces restore               0.00000    0.301511    0.000000\n",
      "gravely injured              0.00000    0.301511    0.000000\n",
      "grimes wakes                 0.27735    0.000000    0.000000\n",
      "group make                   0.00000    0.000000    0.288675\n",
      "group survivors              0.27735    0.000000    0.000000\n",
      "injured construction         0.00000    0.301511    0.000000\n",
      "join forces                  0.00000    0.301511    0.000000\n",
      "lead group                   0.27735    0.000000    0.000000\n",
      "learn world                  0.27735    0.000000    0.000000\n",
      "make risky                   0.00000    0.000000    0.288675\n",
      "need build                   0.00000    0.000000    0.288675\n",
      "restore bridge               0.00000    0.301511    0.000000\n",
      "rick grimes                  0.27735    0.000000    0.000000\n",
      "rick group                   0.00000    0.000000    0.288675\n",
      "risky run                    0.00000    0.000000    0.288675\n",
      "ruins lead                   0.27735    0.000000    0.000000\n",
      "run washington               0.00000    0.000000    0.288675\n",
      "search artifacts             0.00000    0.000000    0.288675\n",
      "sheriff deputy               0.27735    0.000000    0.000000\n",
      "stay alive                   0.27735    0.000000    0.000000\n",
      "survivors stay               0.27735    0.000000    0.000000\n",
      "trade gravely                0.00000    0.301511    0.000000\n",
      "wakes coma                   0.27735    0.000000    0.000000\n",
      "washington search            0.00000    0.000000    0.288675\n",
      "world ruins                  0.27735    0.000000    0.000000\n"
     ]
    }
   ],
   "source": [
    "# 2-gram\n",
    "print(get_tfidf(documents, ngram_range=(2,2), index=document_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Document 0  Document 1  Document 2\n",
      "artifacts need build               0.000000    0.000000    0.301511\n",
      "bridge facilitate communication    0.000000    0.316228    0.000000\n",
      "build civilization carl            0.000000    0.000000    0.301511\n",
      "civilization carl envisioned       0.000000    0.000000    0.301511\n",
      "coma learn world                   0.288675    0.000000    0.000000\n",
      "communication trade gravely        0.000000    0.316228    0.000000\n",
      "communities join forces            0.000000    0.316228    0.000000\n",
      "deputy rick grimes                 0.288675    0.000000    0.000000\n",
      "facilitate communication trade     0.000000    0.316228    0.000000\n",
      "forces restore bridge              0.000000    0.316228    0.000000\n",
      "gravely injured construction       0.000000    0.316228    0.000000\n",
      "grimes wakes coma                  0.288675    0.000000    0.000000\n",
      "group make risky                   0.000000    0.000000    0.301511\n",
      "group survivors stay               0.288675    0.000000    0.000000\n",
      "injured construction site          0.000000    0.316228    0.000000\n",
      "join forces restore                0.000000    0.316228    0.000000\n",
      "lead group survivors               0.288675    0.000000    0.000000\n",
      "learn world ruins                  0.288675    0.000000    0.000000\n",
      "make risky run                     0.000000    0.000000    0.301511\n",
      "need build civilization            0.000000    0.000000    0.301511\n",
      "restore bridge facilitate          0.000000    0.316228    0.000000\n",
      "rick grimes wakes                  0.288675    0.000000    0.000000\n",
      "rick group make                    0.000000    0.000000    0.301511\n",
      "risky run washington               0.000000    0.000000    0.301511\n",
      "ruins lead group                   0.288675    0.000000    0.000000\n",
      "run washington search              0.000000    0.000000    0.301511\n",
      "search artifacts need              0.000000    0.000000    0.301511\n",
      "sheriff deputy rick                0.288675    0.000000    0.000000\n",
      "survivors stay alive               0.288675    0.000000    0.000000\n",
      "trade gravely injured              0.000000    0.316228    0.000000\n",
      "wakes coma learn                   0.288675    0.000000    0.000000\n",
      "washington search artifacts        0.000000    0.000000    0.301511\n",
      "world ruins lead                   0.288675    0.000000    0.000000\n"
     ]
    }
   ],
   "source": [
    "# 3-gram\n",
    "print(get_tfidf(documents, ngram_range=(3,3), index=document_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"aufgabe3.png\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
